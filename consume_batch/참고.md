    async fn process_spent_detail_dynamic(
        schedule_item: &BatchScheduleItem,
        elastic_service: &Arc<E>,
        consume_service: &Arc<C>,
        new_index_name: &str,
    ) -> anyhow::Result<u64> {
        let relation_topic: &str = schedule_item.relation_topic_sub();
        let batch_size: usize = *schedule_item.batch_size();
        let consumer_group: &str = schedule_item.consumer_group();

        info!(
            "[BatchServiceImpl::process_spent_detail_dynamic] Starting. topic='{}', index='{}'",
            relation_topic, new_index_name
        );

        let mut total_processed: u64 = 0;
        let mut empty_count: i32 = 0;

        loop {
            let messages: Vec<SpentDetailWithRelations> = consume_service
                .consume_messages_as_with_group(relation_topic, batch_size, consumer_group)
                .await
                .context(
                    "[BatchServiceImpl::process_spent_detail_dynamic] Failed to consume messages",
                )?;
            

            let max_static_produced_at: DateTime<Utc> =
                get_max_static_spent_detail_index_timestamp().await;
            let filtered: Vec<SpentDetailWithRelations> = messages
                .into_iter()
                .filter(|msg| msg.produced_at > max_static_produced_at)
                .collect();

            if filtered.is_empty() {
                empty_count += 1;
                info!(
                    "[BatchServiceImpl::process_spent_detail_dynamic] No new messages ({}/3 empty batches)",
                    empty_count
                );
                if empty_count > 3 {
                    info!(
                        "[BatchServiceImpl::process_spent_detail_dynamic] Terminating after {} consecutive empty batches",
                        empty_count
                    );
                    break;
                }
                tokio::time::sleep(Duration::from_secs(3)).await;
                continue;
            }

            empty_count = 0;
            let batch_count: usize = filtered.len();

            let max_dynamic_produced_at: DateTime<Utc> =
                get_max_dynamic_spent_detail_index_timestamp().await;

            let cur_max_produced_at: DateTime<Utc> = filtered
                .iter()
                .map(|m| m.produced_at)
                .max()
                .unwrap_or_else(Utc::now);

            let catch_up_complete: bool =
                Self::check_catch_up_status(max_dynamic_produced_at, cur_max_produced_at).await;

            if catch_up_complete {
                set_spent_detail_indexing(false).await;
            }

            info!(
                "[BatchServiceImpl::process_spent_detail_dynamic] Processing {} messages (catch_up_complete={})",
                batch_count, catch_up_complete
            );

            let (to_insert, to_update, to_delete) = Self::partition_by_indexing_type(filtered);

            Self::apply_es_operations(
                elastic_service,
                new_index_name,
                to_insert,
                to_update,
                to_delete,
            )
            .await?;

            total_processed += batch_count as u64;

            if catch_up_complete {
                info!(
                    "[BatchServiceImpl::process_spent_detail_dynamic] Catch-up complete. total_processed={}",
                    total_processed
                );
                break;
            }

            info!(
                "[BatchServiceImpl::process_spent_detail_dynamic] total_processed={} so far",
                total_processed
            );

            tokio::time::sleep(Duration::from_secs(10)).await;
        }

        info!(
            "[BatchServiceImpl::process_spent_detail_dynamic] Done. total_processed={}",
            total_processed
        );

        Ok(total_processed)
    }